---
title: "‡∏ö‡∏ó‡∏ó‡∏µ‡πà 10: Deployment ‡πÅ‡∏•‡∏∞ Best Practices"
description: "‡∏ô‡∏≥ LangChain Application ‡∏Ç‡∏∂‡πâ‡∏ô Production ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏û‡∏£‡πâ‡∏≠‡∏° monitoring, testing, ‡πÅ‡∏•‡∏∞ cost optimization"
sidebar:
  order: 10
---

import {
  Aside,
  Tabs,
  TabItem,
  Steps,
  Card,
  CardGrid,
} from "@astrojs/starlight/components";

## Production Checklist

‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥ LangChain app ‡∏Ç‡∏∂‡πâ‡∏ô production ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

- ‚úÖ API Keys ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô environment variables
- ‚úÖ ‡∏°‡∏µ error handling ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
- ‚úÖ ‡∏°‡∏µ rate limiting
- ‚úÖ ‡∏°‡∏µ logging ‡πÅ‡∏•‡∏∞ monitoring
- ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö edge cases ‡πÅ‡∏•‡πâ‡∏ß
- ‚úÖ ‡∏°‡∏µ fallback models
- ‚úÖ ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° cost ‡πÑ‡∏î‡πâ

## Release Update Checklist

‡πÉ‡∏ä‡πâ checklist ‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡∏â‡∏ö‡∏±‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‚Äù ‡πÇ‡∏î‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏≤‡∏¢‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™

1. ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï baseline version ‡πÉ‡∏ô‡∏ö‡∏ó setup ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏ö‡∏ö‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡πÄ‡∏ä‡πà‡∏ô `2026-02-13`)
2. ‡∏£‡∏±‡∏ô `npm run check:docs` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Å frontmatter, code fence ‡πÅ‡∏•‡∏∞ placeholder ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏´‡∏•‡∏∏‡∏î
3. ‡∏£‡∏±‡∏ô `npm run build` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ build ‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏ô‡πâ‡∏≤
4. ‡∏ï‡∏£‡∏ß‡∏à‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á API/model ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏≠‡∏Å‡∏≤‡∏™‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠ policy ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á
5. ‡πÄ‡∏õ‡∏¥‡∏î‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏ö‡∏ó production ‡∏ó‡∏±‡πâ‡∏á‡∏ö‡∏ó‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ deployment, monitoring, testing ‡∏¢‡∏±‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö baseline
6. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡∏•‡∏á `build_fix.txt` ‡∏´‡∏£‡∏∑‡∏≠ changelog ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á

<Aside type="tip">
  ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡∏° ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á CI ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏õ‡πá‡∏ô `npm run check && npm run build` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡πà‡∏≠‡∏ô merge
</Aside>

---

## LangServe ‚Äî Deploy ‡πÄ‡∏õ‡πá‡∏ô REST API

### ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```bash
pip install langserve[all] fastapi uvicorn
```

### ‡∏™‡∏£‡πâ‡∏≤‡∏á API Server

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python title="server.py"
from fastapi import FastAPI
from langserve import add_routes
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ‡∏™‡∏£‡πâ‡∏≤‡∏á chain
prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô {topic}"),
    ("human", "{question}"),
])
llm = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | llm | StrOutputParser()

# ‡∏™‡∏£‡πâ‡∏≤‡∏á FastAPI app
app = FastAPI(
    title="LangChain API",
    version="1.0",
    description="LangChain Powered API",
)

# ‡πÄ‡∏û‡∏¥‡πà‡∏° routes
add_routes(app, chain, path="/chat")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```bash
# ‡∏£‡∏±‡∏ô server
python server.py

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
curl -X POST http://localhost:8000/chat/invoke \
  -H "Content-Type: application/json" \
  -d '{"input": {"topic": "Python", "question": "Generator ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"}}'
```

<Aside type="tip">
  LangServe ‡∏™‡∏£‡πâ‡∏≤‡∏á **playground** ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏µ‡πà
  `http://localhost:8000/chat/playground` ‚Äî ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö!
</Aside>

---

## Docker Deployment

### Dockerfile

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: Dockerfile ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö build ‡πÅ‡∏•‡∏∞ deploy ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```dockerfile title="Dockerfile"
FROM python:3.12-slim

WORKDIR /app

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Expose port
EXPOSE 8000

# Run
CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8000"]
```

### docker-compose.yml

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≠‡∏ô‡∏ü‡∏¥‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ñ‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```yaml title="docker-compose.yml"
version: "3.8"

services:
  langchain-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      - LANGSMITH_TRACING=true
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

volumes:
  redis-data:
```

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```bash
# Build ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô
docker-compose up -d

# ‡∏î‡∏π logs
docker-compose logs -f langchain-api
```

---

## LangSmith ‚Äî Monitoring & Debugging

### ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LangSmith

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ

```bash title=".env"
LANGSMITH_API_KEY=lsv2_xxxxxxxxx
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=my-production-app
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
```

### Trace Calls

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ‡∏ó‡∏∏‡∏Å call ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å trace ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LANGSMITH_TRACING=true
chain = prompt | llm | parser
result = chain.invoke(
    {"question": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ"},
    config={"run_name": "greeting-chain"},  # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ run
)
```

### Custom Tracing

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langsmith import traceable

@traceable(name="process_document")
def process_document(doc_path: str) -> str:
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‚Äî ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å trace ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    # ... ‡πÇ‡∏Ñ‡πâ‡∏î‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    return result

@traceable(name="generate_response")
def generate_response(question: str, context: str) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‚Äî ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å trace ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
    chain = prompt | llm | parser
    return chain.invoke({"question": question, "context": context})
```

<Aside type="note">
  LangSmith ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô request/response ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å LLM call, ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ, token
  usage, ‡πÅ‡∏•‡∏∞ cost ‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
</Aside>

---

## Cost Optimization

### 1. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI

# ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‚Üí model ‡∏ñ‡∏π‡∏Å
simple_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ‡∏á‡∏≤‡∏ô‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‚Üí model ‡πÅ‡∏û‡∏á
complex_llm = ChatOpenAI(model="gpt-4o", temperature=0)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
def smart_route(question: str):
    if len(question) < 50 and "?" in question:
        return simple_llm  # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢
    return complex_llm     # ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
```

### 2. Caching

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.globals import set_llm_cache
from langchain_community.cache import SQLiteCache

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ cache
set_llm_cache(SQLiteCache(database_path=".langchain.db"))

# ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏° ‚Üí ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API ‡∏ã‡πâ‡∏≥
result1 = llm.invoke("Python ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?")  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
result2 = llm.invoke("Python ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?")  # ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å cache (‡∏ü‡∏£‡∏µ!)
```

### 3. Monitor Token Usage

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_community.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = chain.invoke({"question": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ AI"})

print(f"Tokens used: {cb.total_tokens}")
print(f"Prompt tokens: {cb.prompt_tokens}")
print(f"Completion tokens: {cb.completion_tokens}")
print(f"Cost: ${cb.total_cost:.4f}")
```

---

## Error Handling

### Retry ‡πÅ‡∏•‡∏∞ Fallback Pattern

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI

# Primary + Fallback
primary = ChatOpenAI(model="gpt-4o")
fallback = ChatOpenAI(model="gpt-4o-mini")

# Retry 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏•‡πâ‡∏ß fallback
robust_llm = primary.with_retry(
    stop_after_attempt=3
).with_fallbacks([fallback])

# ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô chain
chain = prompt | robust_llm | parser
```

### Timeout

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
llm = ChatOpenAI(
    model="gpt-4o-mini",
    timeout=30,       # timeout 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    max_retries=2,    # retry ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
)
```

### Rate Limiting

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
import time
from functools import wraps

def rate_limit(calls_per_minute: int):
    """‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API"""
    interval = 60.0 / calls_per_minute
    last_call = [0]

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_call[0]
            if elapsed < interval:
                time.sleep(interval - elapsed)
            last_call[0] = time.time()
            return func(*args, **kwargs)
        return wrapper
    return decorator

@rate_limit(calls_per_minute=30)
def call_llm(question: str) -> str:
    return chain.invoke({"question": question})
```

---

## Testing

### Unit Testing

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python title="test_chains.py"
import pytest
from unittest.mock import patch, MagicMock
from langchain_core.messages import AIMessage

def test_chain_output():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ chain ‡πÉ‡∏´‡πâ output ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
    with patch("langchain_openai.ChatOpenAI") as mock_llm:
        mock_llm.return_value.invoke.return_value = AIMessage(
            content="Python ‡∏Ñ‡∏∑‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°"
        )
        result = chain.invoke({"question": "Python ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"})
        assert "Python" in result
        assert len(result) > 0

def test_chain_with_empty_input():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö edge case: input ‡∏ß‡πà‡∏≤‡∏á"""
    result = chain.invoke({"question": ""})
    assert result is not None
```

### Integration Testing

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
def test_rag_pipeline():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG pipeline ‡πÅ‡∏ö‡∏ö end-to-end"""
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á test documents
    test_docs = [
        Document(page_content="LangChain ‡πÄ‡∏õ‡πá‡∏ô framework ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM"),
    ]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store
    vectorstore = Chroma.from_documents(test_docs, embeddings)
    retriever = vectorstore.as_retriever()

    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö retrieval
    results = retriever.invoke("LangChain ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?")
    assert len(results) > 0
    assert "LangChain" in results[0].page_content
```

---

## ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Production Project

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏ü‡∏•‡∏ß‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

```mermaid
graph TD
    Root["my-langchain-app/"]
    Root --> Env[".env (API Keys)"]
    Root --> EnvEx[".env.example"]
    Root --> Compose["docker-compose.yml"]
    Root --> Dockerfile["Dockerfile"]
    Root --> Req["requirements.txt"]
    Root --> Pyproject["pyproject.toml"]
    Root --> Readme["README.md"]

    Root --> Src["src/"]
    Src --> SrcInit["__init__.py"]
    Src --> Server["server.py (FastAPI + LangServe)"]
    Src --> Config["config.py (Configuration)"]
    Src --> Chains["chains/"]
    Chains --> ChainsInit["__init__.py"]
    Chains --> Chat["chat.py"]
    Chains --> Rag["rag.py"]
    Src --> Agents["agents/"]
    Agents --> AgentsInit["__init__.py"]
    Agents --> Assistant["assistant.py"]
    Src --> Tools["tools/"]
    Tools --> ToolsInit["__init__.py"]
    Tools --> Custom["custom.py"]
    Src --> Utils["utils/"]
    Utils --> UtilsInit["__init__.py"]
    Utils --> Callbacks["callbacks.py"]

    Root --> Tests["tests/"]
    Tests --> TInit["__init__.py"]
    Tests --> TChains["test_chains.py"]
    Tests --> TAgents["test_agents.py"]
    Tests --> TRag["test_rag.py"]

    Root --> Data["data/"]
    Data --> Documents["documents/"]
    Data --> Vectorstore["vectorstore/"]
```

---

:::note[‡∏Å‡πâ‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ üíª]
‡πÑ‡∏õ‡∏î‡∏π **[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 11: ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á](/04-production/02-projects/)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏à‡∏£‡∏¥‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ!
:::
