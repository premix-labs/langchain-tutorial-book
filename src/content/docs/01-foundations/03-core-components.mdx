---
title: "‡∏ö‡∏ó‡∏ó‡∏µ‡πà 3: Components ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á LangChain"
description: "‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à LLMs, Chat Models, Messages ‡πÅ‡∏•‡∏∞ Core Components ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á AI Application"
sidebar:
  order: 3
---

import { Aside, Tabs, TabItem, Card, CardGrid } from '@astrojs/starlight/components';

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° Core Components

LangChain ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ components ‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô:

```mermaid
graph TD
    Core["LangChain Core"]
    Core --> Models["Models<br/>(LLM/Chat)"]
    Core --> Prompts["Prompts<br/>(Templates)"]
    Core --> Parsers["Output<br/>Parsers"]
    Core --> Runnable["Runnable<br/>(LCEL)"]
```

---

## 1. Language Models

LangChain ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å:

### LLMs (Text Completion)

LLMs ‡∏£‡∏±‡∏ö **string** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á **string** ‡∏≠‡∏≠‡∏Å:

```python
from langchain_openai import OpenAI

# Text completion model
llm = OpenAI(model="gpt-3.5-turbo-instruct")
result = llm.invoke("‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á")
print(result)
# ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏¥‡∏°‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥‡πÄ‡∏à‡πâ‡∏≤‡∏û‡∏£‡∏∞‡∏¢‡∏≤...
```

### Chat Models (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‚úÖ)

Chat Models ‡∏£‡∏±‡∏ö **messages** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á **message** ‡∏≠‡∏≠‡∏Å ‚Äî ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Chat Model
chat = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,     # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (0=‡πÄ‡∏õ‡πä‡∏∞, 1=‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå)
    max_tokens=1000,     # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
)

# ‡∏™‡πà‡∏á messages
messages = [
    SystemMessage(content="‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô Python"),
    HumanMessage(content="‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ List Comprehension ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢")
]

response = chat.invoke(messages)
print(response.content)
```

<Aside type="tip">
  **Chat Models ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô** ‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô LangChain ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Chat Models ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö features ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô Tool Calling, structured output
</Aside>

### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö LLM Providers

| Provider | Model | ‡∏£‡∏≤‡∏Ñ‡∏≤ | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô |
|----------|-------|------|---------|
| **OpenAI** | gpt-4o-mini | ‡∏ñ‡∏π‡∏Å | ‡πÄ‡∏£‡πá‡∏ß, ‡∏≠‡πÄ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå |
| **OpenAI** | gpt-4o | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | ‡∏â‡∏•‡∏≤‡∏î‡∏°‡∏≤‡∏Å, multimodal |
| **Google** | gemini-2.0-flash | ‡∏ü‡∏£‡∏µ-‡∏ñ‡∏π‡∏Å | ‡πÄ‡∏£‡πá‡∏ß, context ‡∏¢‡∏≤‡∏ß |
| **Anthropic** | claude-3.5-sonnet | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á | ‡πÄ‡∏Å‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î |
| **Meta** | llama-3.1 | ‡∏ü‡∏£‡∏µ (self-host) | Open source |

---

## 2. Messages

‡∏£‡∏∞‡∏ö‡∏ö Messages ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á Chat Models:

```python
from langchain_core.messages import (
    SystemMessage,    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó/‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å AI
    HumanMessage,     # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    AIMessage,        # ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
    ToolMessage,      # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Tool
)
```

### ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Messages

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
messages = [
    # 1. SystemMessage - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
    SystemMessage(content="""
        ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢
        ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏´‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    """),

    # 2. HumanMessage - ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    HumanMessage(content="‡∏™‡∏π‡∏ï‡∏£‡∏ú‡∏±‡∏î‡πÑ‡∏ó‡∏¢‡∏ó‡∏≥‡∏¢‡∏±‡∏á‡πÑ‡∏á?"),

    # 3. AIMessage - ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö context)
    AIMessage(content="‡∏ú‡∏±‡∏î‡πÑ‡∏ó‡∏¢‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡πâ‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡πå..."),

    # 4. HumanMessage - ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
    HumanMessage(content="‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏™‡πâ‡∏ô‡∏à‡∏±‡∏ô‡∏ó‡πå ‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡πÅ‡∏ó‡∏ô‡πÑ‡∏î‡πâ?"),
]

response = chat.invoke(messages)
```

<Aside type="note">
  **SystemMessage** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤ AI ‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á persona ‡πÉ‡∏´‡πâ chatbot
</Aside>

---

## 3. Prompt Templates

Prompt Templates ‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á prompts ‡πÅ‡∏ö‡∏ö dynamic:

### ChatPromptTemplate (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.prompts import ChatPromptTemplate

# ‡∏™‡∏£‡πâ‡∏≤‡∏á template
prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô {topic}"),
    ("human", "{question}"),
])

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô template
messages = prompt.invoke({
    "topic": "Machine Learning",
    "question": "Neural Network ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"
})

# ‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á LLM
response = chat.invoke(messages)
print(response.content)
```

### PromptTemplate (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö text)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.prompts import PromptTemplate

# ‡∏™‡∏£‡πâ‡∏≤‡∏á template
template = PromptTemplate.from_template("""
‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å {source_lang} ‡πÄ‡∏õ‡πá‡∏ô {target_lang}:

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {text}

‡∏Ñ‡∏≥‡πÅ‡∏õ‡∏•:
""")

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
result = template.invoke({
    "source_lang": "‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©",
    "target_lang": "‡πÑ‡∏ó‡∏¢",
    "text": "Hello, how are you?"
})

print(result.text)
```

---

## 4. Output Parsers

Output Parsers ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å LLM ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:

### StrOutputParser

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.output_parsers import StrOutputParser

# ‡πÅ‡∏õ‡∏•‡∏á AIMessage ‡πÄ‡∏õ‡πá‡∏ô string
parser = StrOutputParser()

# ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö chain (‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô‡∏ö‡∏ó‡∏ó‡∏µ‡πà 5)
chain = prompt | chat | parser
result = chain.invoke({"topic": "Python", "question": "Lambda ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?"})
print(result)  # ‡πÑ‡∏î‡πâ string ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
```

### JsonOutputParser

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
class Recipe(BaseModel):
    name: str = Field(description="‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏°‡∏ô‡∏π")
    ingredients: list[str] = Field(description="‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏î‡∏¥‡∏ö")
    cooking_time: int = Field(description="‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏≥ (‡∏ô‡∏≤‡∏ó‡∏µ)")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á parser
parser = JsonOutputParser(pydantic_object=Recipe)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å format
prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏ü‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏ó‡∏¢ ‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö JSON\n{format_instructions}"),
    ("human", "‡∏™‡∏π‡∏ï‡∏£ {dish}"),
])

# ‡∏î‡∏∂‡∏á format instructions
prompt_with_format = prompt.partial(
    format_instructions=parser.get_format_instructions()
)

chain = prompt_with_format | chat | parser
result = chain.invoke({"dish": "‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á"})
print(result)
# {'name': '‡∏ï‡πâ‡∏°‡∏¢‡∏≥‡∏Å‡∏∏‡πâ‡∏á', 'ingredients': [...], 'cooking_time': 30}
```

---

## 5. Structured Output (with_structured_output)

‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏î‡πâ structured data:

```python
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI

class MovieReview(BaseModel):
    """‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå"""
    title: str = Field(description="‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå")
    rating: float = Field(description="‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 1-10")
    summary: str = Field(description="‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ")
    recommend: bool = Field(description="‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")

# ‡πÉ‡∏ä‡πâ with_structured_output
llm = ChatOpenAI(model="gpt-4o-mini")
structured_llm = llm.with_structured_output(MovieReview)

result = structured_llm.invoke("‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏´‡∏ô‡∏±‡∏á Inception ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢")
print(f"‡∏ä‡∏∑‡πà‡∏≠: {result.title}")
print(f"‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {result.rating}/10")
print(f"‡∏™‡∏£‡∏∏‡∏õ: {result.summary}")
print(f"‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: {'‚úÖ' if result.recommend else '‚ùå'}")
```

<Aside type="tip">
  `with_structured_output()` ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥** ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö structured output ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ **Tool Calling** ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤ JSON parsing
</Aside>

---

## 6. ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ invoke, stream, batch

LangChain ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 3 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# 1. invoke - ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏£‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
result = llm.invoke("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ")
print(result.content)

# 2. stream - ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô (real-time)
for chunk in llm.stream("‡πÄ‡∏•‡πà‡∏≤‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢"):
    print(chunk.content, end="", flush=True)

# 3. batch - ‡∏™‡πà‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
results = llm.batch([
    "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢",
    "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô",
    "‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏´‡∏•‡∏ß‡∏á‡∏Ç‡∏≠‡∏á‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™",
])
for r in results:
    print(r.content)
```

### Async versions

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
import asyncio

async def main():
    llm = ChatOpenAI(model="gpt-4o-mini")

    # ainvoke
    result = await llm.ainvoke("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ")
    print(result.content)

    # astream
    async for chunk in llm.astream("‡πÄ‡∏•‡πà‡∏≤‡∏ô‡∏¥‡∏ó‡∏≤‡∏ô"):
        print(chunk.content, end="")

    # abatch
    results = await llm.abatch(["‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° 1", "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° 2"])

asyncio.run(main())
```

---

## ‡∏™‡∏£‡∏∏‡∏õ

<CardGrid>
  <Card title="Models" icon="laptop">
    ‡πÉ‡∏ä‡πâ ChatOpenAI ‡∏´‡∏£‡∏∑‡∏≠ ChatGoogleGenerativeAI ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
  </Card>
  <Card title="Messages" icon="comment">
    System, Human, AI Messages ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
  </Card>
  <Card title="Prompts" icon="document">
    ChatPromptTemplate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö dynamic prompts
  </Card>
  <Card title="Output" icon="list-format">
    StrOutputParser, JsonOutputParser, ‡∏´‡∏£‡∏∑‡∏≠ with_structured_output
  </Card>
</CardGrid>

---

:::note[‡∏Å‡πâ‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ üöÄ]
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Components ‡∏´‡∏•‡∏±‡∏Å‡πÅ‡∏•‡πâ‡∏ß ‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á **[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 4: Prompt Templates](/02-core/01-prompts/)** ‡∏Å‡∏±‡∏ô!
:::
