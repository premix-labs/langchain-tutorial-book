---
title: "‡∏ö‡∏ó‡∏ó‡∏µ‡πà 5: Chains ‡πÅ‡∏•‡∏∞ LCEL (LangChain Expression Language)"
description: "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á chains ‡∏î‡πâ‡∏ß‡∏¢ LCEL ‚Äî ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á LangChain ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö composable AI workflows"
sidebar:
  order: 5
---

import {
  Aside,
  Tabs,
  TabItem,
  Steps,
  Card,
  CardGrid,
} from "@astrojs/starlight/components";

## LCEL ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**LangChain Expression Language (LCEL)** ‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á chains ‡πÉ‡∏ô LangChain ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **pipe operator (`|`)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ components ‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô

## LCEL Syntax

‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á LCEL ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡πà‡∏≠ (`|`) ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏≠‡∏µ‡∏Å‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô UNIX pipe

```mermaid
graph LR
    Input([Input]) --> Prompt[Prompt Template]
    Prompt --> Model[LLM / ChatModel]
    Model --> Parser[Output Parser]
    Parser --> Output([Output Result])
```

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
chain = prompt | model | output_parser
#        ‚Üì        ‚Üì       ‚Üì
#     ‡∏™‡∏£‡πâ‡∏≤‡∏á     ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ   ‡πÅ‡∏õ‡∏•‡∏á
#     prompt    LLM      ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
```

<Aside type="note">
  LCEL ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÉ‡∏ô LangChain v1.0+ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏ó‡∏ô chain API ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤
  ‡πÄ‡∏ä‡πà‡∏ô `LLMChain`, `SequentialChain`
</Aside>

---

## Chain ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á components
prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô {topic}"),
    ("human", "{question}"),
])

llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á chain ‡∏î‡πâ‡∏ß‡∏¢ LCEL
chain = prompt | llm | parser

# 3. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
result = chain.invoke({
    "topic": "Python",
    "question": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ decorator ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢"
})
print(result)
```

### ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Chain

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÅ‡∏ú‡∏ô‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏ü‡∏•‡∏ß‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

```mermaid
graph LR
    Input["invoke({topic, question})"] --> Prompt["Prompt Template"]
    Prompt --> LLM["LLM"]
    LLM --> Parser["Parser"]
    Parser --> Output["‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (string)"]

    InputNote["dict -> messages"] -.-> Prompt
    LLMNote["messages -> AIMessage"] -.-> LLM
    ParserNote["AIMessage -> string"] -.-> Parser
```

---

## RunnableLambda ‚Äî Custom Functions

‡πÉ‡∏™‡πà **function ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô chain:

```python
from langchain_core.runnables import RunnableLambda

# Custom function
def format_output(text: str) -> str:
    """‡πÄ‡∏û‡∏¥‡πà‡∏° emoji ‡πÅ‡∏•‡∏∞ formatting"""
    lines = text.strip().split("\n")
    formatted = []
    for i, line in enumerate(lines, 1):
        formatted.append(f"üìå {i}. {line.strip()}")
    return "\n".join(formatted)

# ‡πÉ‡∏™‡πà‡πÉ‡∏ô chain
chain = prompt | llm | StrOutputParser() | RunnableLambda(format_output)
result = chain.invoke({
    "topic": "Python",
    "question": "‡∏ö‡∏≠‡∏Å 5 library ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ"
})
print(result)
# üìå 1. NumPy - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
# üìå 2. Pandas - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ...
```

---

## RunnablePassthrough ‚Äî ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnablePassthrough

# ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ input ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á
chain = RunnablePassthrough() | prompt | llm | parser

# assign ‚Äî ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô input
chain = RunnablePassthrough.assign(
    word_count=lambda x: len(x["text"].split())
) | prompt | llm | parser
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ prompt

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnablePassthrough
from datetime import datetime

def add_metadata(input_dict):
    """‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô input"""
    return {
        **input_dict,
        "current_date": datetime.now().strftime("%Y-%m-%d"),
        "language": "Thai",
    }

prompt = ChatPromptTemplate.from_messages([
    ("system", "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {current_date}. ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤{language}"),
    ("human", "{question}"),
])

chain = RunnableLambda(add_metadata) | prompt | llm | parser
result = chain.invoke({"question": "‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î?"})
```

---

## RunnableParallel ‚Äî ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnableParallel

# ‡∏™‡∏£‡πâ‡∏≤‡∏á chains ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
summary_chain = summary_prompt | llm | parser
sentiment_chain = sentiment_prompt | llm | parser
keywords_chain = keywords_prompt | llm | parser

# ‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô!
parallel_chain = RunnableParallel(
    summary=summary_chain,
    sentiment=sentiment_chain,
    keywords=keywords_chain,
)

result = parallel_chain.invoke({"text": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå..."})
print(result["summary"])
print(result["sentiment"])
print(result["keywords"])
```

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏¥‡∏ï‡∏¥

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

# 3 chains ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
analyzer = RunnableParallel(
    summary=ChatPromptTemplate.from_template(
        "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏ô 2 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ: {text}"
    ) | llm | parser,

    mood=ChatPromptTemplate.from_template(
        "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ö‡∏ß‡∏Å/‡∏•‡∏ö/‡∏Å‡∏•‡∏≤‡∏á): {text}"
    ) | llm | parser,

    topics=ChatPromptTemplate.from_template(
        "‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (3 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠): {text}"
    ) | llm | parser,
)

result = analyzer.invoke({
    "text": "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ LangChain ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å ‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏°‡∏≤‡∏Å API ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏µ"
})
print(f"üìù ‡∏™‡∏£‡∏∏‡∏õ: {result['summary']}")
print(f"üòä ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå: {result['mood']}")
print(f"üè∑Ô∏è ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠: {result['topics']}")
```

---

## Branching ‚Äî ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Chain ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnableBranch

# ‡∏™‡∏£‡πâ‡∏≤‡∏á branch ‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
branch = RunnableBranch(
    # (‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç, chain ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏±‡∏ô)
    (lambda x: "‡πÅ‡∏õ‡∏•" in x["task"], translate_chain),
    (lambda x: "‡∏™‡∏£‡∏∏‡∏õ" in x["task"], summarize_chain),
    (lambda x: "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" in x["task"], analyze_chain),
    # default chain (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÑ‡∏´‡∏ô‡πÄ‡∏•‡∏¢)
    general_chain,
)

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
result = branch.invoke({"task": "‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ", "text": "Hello World"})
```

---

## Streaming ‚Äî ‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ö‡∏ö Real-time

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
chain = prompt | llm | parser

# Stream ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
for chunk in chain.stream({
    "topic": "‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£",
    "question": "‡∏™‡∏π‡∏ï‡∏£‡∏ú‡∏±‡∏î‡∏Å‡∏∞‡πÄ‡∏û‡∏£‡∏≤‡∏´‡∏°‡∏π‡∏™‡∏±‡∏ö"
}):
    print(chunk, end="", flush=True)
```

### Async Streaming

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
import asyncio

async def stream_response():
    chain = prompt | llm | parser

    async for chunk in chain.astream({
        "topic": "Python",
        "question": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ async/await"
    }):
        print(chunk, end="", flush=True)

asyncio.run(stream_response())
```

---

## Chain ‡∏ã‡πâ‡∏≠‡∏ô Chain

### Sequential Chain ‚Äî ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
# Chain 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏£‡πà‡∏≤‡∏á
outline_prompt = ChatPromptTemplate.from_template(
    "‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏£‡πà‡∏≤‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á: {topic}\n‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô 5 ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢"
)
outline_chain = outline_prompt | llm | parser

# Chain 2: ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏£‡πà‡∏≤‡∏á
article_prompt = ChatPromptTemplate.from_template(
    "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏£‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ:\n{outline}\n\n‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÅ‡∏ï‡πà‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö"
)
article_chain = article_prompt | llm | parser

# ‡∏£‡∏ß‡∏° chain
full_chain = (
    {"outline": outline_chain, "topic": RunnablePassthrough()}
    | RunnableLambda(lambda x: {"outline": x["outline"]})
    | article_chain
)

result = full_chain.invoke({"topic": "AI ‡πÉ‡∏ô‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏≠‡∏≤‡∏´‡∏≤‡∏£"})
```

### Pipeline Pattern (‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnablePassthrough

# Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
generate_questions = ChatPromptTemplate.from_template(
    "‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ '{topic}' ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° 3 ‡∏Ç‡πâ‡∏≠"
) | llm | parser

# Step 2: ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
answer_questions = ChatPromptTemplate.from_template(
    "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:\n{questions}"
) | llm | parser

# Step 3: ‡∏™‡∏£‡∏∏‡∏õ
summarize = ChatPromptTemplate.from_template(
    "‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß:\n{answers}"
) | llm | parser

# Pipeline
pipeline = (
    RunnablePassthrough.assign(questions=generate_questions)
    | RunnablePassthrough.assign(answers=answer_questions)
    | summarize
)

result = pipeline.invoke({"topic": "Quantum Computing"})
print(result)
```

---

## Error Handling ‚Äî Fallback Chains

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI

# Primary model (‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÅ‡∏û‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡πâ‡∏≤)
primary_llm = ChatOpenAI(model="gpt-4o")

# Fallback model (‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤, ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)
fallback_llm = ChatOpenAI(model="gpt-4o-mini")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á chain ‡∏û‡∏£‡πâ‡∏≠‡∏° fallback
chain = prompt | primary_llm.with_fallbacks([fallback_llm]) | parser

# ‡∏ñ‡πâ‡∏≤ gpt-4o ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏à‡∏∞‡πÉ‡∏ä‡πâ gpt-4o-mini ‡πÅ‡∏ó‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
result = chain.invoke({"topic": "AI", "question": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Transformer"})
```

### Retry Logic

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
# Retry ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
chain = prompt | llm.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
) | parser
```

---

## RunnableConfig ‚Äî ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(
    max_concurrency=5,    # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
    run_name="my-chain",  # ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring
    tags=["production"],  # tags ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LangSmith
    metadata={"user_id": "123"},
)

result = chain.invoke(input_data, config=config)
```

---

## ‡∏™‡∏£‡∏∏‡∏õ LCEL Components

| Component             | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà              | Syntax                                 |
| --------------------- | -------------------- | -------------------------------------- |
| `\|` (pipe)           | ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ components | `a \| b \| c`                          |
| `RunnableLambda`      | Custom function      | `RunnableLambda(fn)`                   |
| `RunnablePassthrough` | ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ input         | `RunnablePassthrough()`                |
| `RunnableParallel`    | ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô        | `RunnableParallel(a=..., b=...)`       |
| `RunnableBranch`      | ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç     | `RunnableBranch((cond, chain), ...)`   |
| `.with_fallbacks()`   | ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ error         | `llm.with_fallbacks([backup])`         |
| `.with_retry()`       | ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà              | `llm.with_retry(stop_after_attempt=3)` |

---

:::note[‡∏Å‡πâ‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ üß†]
‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 6: Memory](/02-core/03-memory/)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏à‡∏≥‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏î‡πâ!
:::
