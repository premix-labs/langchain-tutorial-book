---
title: "‡∏ö‡∏ó‡∏ó‡∏µ‡πà 7: RAG ‚Äî Retrieval Augmented Generation"
description: "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á ‡∏î‡πâ‡∏ß‡∏¢ RAG Pipeline ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
sidebar:
  order: 7
---

import {
  Aside,
  Tabs,
  TabItem,
  Steps,
  Card,
  CardGrid,
} from "@astrojs/starlight/components";

## RAG ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**Retrieval Augmented Generation (RAG)** ‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ LLM ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á** ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á fine-tune model

```mermaid
graph LR
    Q["‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ"] --> R["‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"]
    R --> A["‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö prompt"]
    A --> LLM["LLM"]
    LLM --> Ans["‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥"]

    RNote["Retrieval"] -.-> R
    ANote["Augmentation"] -.-> A
```

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á RAG?

| ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á LLM         | RAG ‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ                     |
| -------------------- | ------------------------------ |
| ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢     | ‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |
| ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£ | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ        |
| Hallucination        | ‚úÖ ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡∏°‡∏µ source  |
| ‡∏Ñ‡πà‡∏≤ Fine-tuning ‡πÅ‡∏û‡∏á  | ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á fine-tune           |

---

## RAG Pipeline Overview

‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 2 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠ **Indexing** (‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•) ‡πÅ‡∏•‡∏∞ **Retrieval & Generation** (‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö)

```mermaid
graph TD
    subgraph Indexing [Phase 1: Indexing]
        direction TB
        Docs[Documents] --> Split[Text Splitter]
        Split --> Chunks[Small Chunks]
        Chunks --> Embed[Embedding Model]
        Embed --> VectorDB[(Vector Store)]
    end

    subgraph Retrieval [Phase 2: Retrieval & Generation]
        direction TB
        Query([User Query]) --> EmbedQ[Embedding Model]
        EmbedQ --> VectorQ[Query Vector]
        VectorQ --> VectorDB
        VectorDB --> Context[Relevant Context]

        Context --> LLM
        Query --> LLM
        LLM --> Answer([Final Answer])
    end
```

## RAG Pipeline

<Steps>

1. **Load** ‚Äî ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
2. **Split** ‚Äî ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡πÜ
3. **Embed** ‚Äî ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô vectors
4. **Store** ‚Äî ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á Vector Store
5. **Retrieve** ‚Äî ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
6. **Generate** ‚Äî ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ LLM ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

</Steps>

---

## Step 1: Document Loaders

### ‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
# PDF
from langchain_community.document_loaders import PyPDFLoader
docs = PyPDFLoader("report.pdf").load()

# Text file
from langchain_community.document_loaders import TextLoader
docs = TextLoader("notes.txt", encoding="utf-8").load()

# CSV
from langchain_community.document_loaders import CSVLoader
docs = CSVLoader("data.csv").load()

# Web page
from langchain_community.document_loaders import WebBaseLoader
docs = WebBaseLoader("https://example.com/article").load()

# Directory (‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå)
from langchain_community.document_loaders import DirectoryLoader
docs = DirectoryLoader("./documents/", glob="**/*.pdf").load()
```

### Document Object

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
# ‡πÅ‡∏ï‡πà‡∏•‡∏∞ document ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á:
doc = docs[0]
print(doc.page_content)  # ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
print(doc.metadata)      # {'source': 'report.pdf', 'page': 0}
```

---

## Step 2: Text Splitters

‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡πÜ (chunks) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ‡∏™‡∏£‡πâ‡∏≤‡∏á text splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,       # ‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk (‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
    chunk_overlap=200,     # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
    separators=["\n\n", "\n", ".", " "],  # ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á
)

# ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
chunks = splitter.split_documents(docs)
print(f"‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏î‡πâ {len(chunks)} chunks")
```

<Aside type="tip">
  **chunk_overlap** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å! ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡∏£‡∏á‡∏£‡∏≠‡∏¢‡∏ï‡πà‡∏≠‡∏Ç‡∏≠‡∏á chunks
  ‡πÇ‡∏î‡∏¢‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÉ‡∏ä‡πâ 10-20% ‡∏Ç‡∏≠‡∏á chunk_size
</Aside>

### ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Splitter ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

| Splitter                         | ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö             |
| -------------------------------- | -------------------- |
| `RecursiveCharacterTextSplitter` | ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) |
| `MarkdownHeaderTextSplitter`     | Markdown files       |
| `PythonCodeTextSplitter`         | Python source code   |
| `HTMLSectionSplitter`            | HTML pages           |
| `TokenTextSplitter`              | ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô tokens   |

---

## Step 3: Embeddings

**Embeddings** ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô vector (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:

```python
from langchain_openai import OpenAIEmbeddings

# ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings model
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô vector
vector = embeddings.embed_query("LangChain ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?")
print(f"‡∏°‡∏¥‡∏ï‡∏¥: {len(vector)}")   # 1536 ‡∏°‡∏¥‡∏ï‡∏¥
print(f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: {vector[:5]}")  # [0.012, -0.034, ...]
```

### ‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Embeddings

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
# Google
from langchain_google_genai import GoogleGenerativeAIEmbeddings
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# Hugging Face (‡∏ü‡∏£‡∏µ, ‡∏£‡∏±‡∏ô‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)
from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

---

## Step 4: Vector Stores

‡πÄ‡∏Å‡πá‡∏ö vectors ‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö semantic:

### ChromaDB (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_community.vectorstores import Chroma

# ‡∏™‡∏£‡πâ‡∏≤‡∏á vector store ‡∏à‡∏≤‡∏Å documents
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    collection_name="my_docs",
    persist_directory="./chroma_db",  # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á disk
)

# ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
results = vectorstore.similarity_search("LangChain ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?", k=3)
for doc in results:
    print(doc.page_content[:100])
    print(f"Source: {doc.metadata['source']}")
    print("---")
```

### FAISS (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å, ‡πÉ‡∏ä‡πâ memory)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_community.vectorstores import FAISS

# ‡∏™‡∏£‡πâ‡∏≤‡∏á
vectorstore = FAISS.from_documents(chunks, embeddings)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î
vectorstore.save_local("faiss_index")
vectorstore = FAISS.load_local("faiss_index", embeddings,
                                allow_dangerous_deserialization=True)
```

---

## Step 5-6: ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain

### RAG Chain ‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# 1. Setup
llm = ChatOpenAI(model="gpt-4o-mini")
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 2. ‡πÇ‡∏´‡∏•‡∏î vector store
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings,
)

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4},  # ‡∏î‡∏∂‡∏á 4 documents
)

# 4. RAG Prompt
prompt = ChatPromptTemplate.from_template("""
‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÄ‡∏™‡∏°‡∏≠

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
""")

# 5. Helper function
def format_docs(docs):
    return "\n\n---\n\n".join(
        f"[Source: {doc.metadata.get('source', 'unknown')}]\n{doc.page_content}"
        for doc in docs
    )

# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 7. ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!
answer = rag_chain.invoke("‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏≤‡∏´‡∏¢‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?")
print(answer)
```

---

## ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ RAG ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

### 1. Multi-Query Retriever

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏∂‡πâ‡∏ô:

```python
from langchain.retrievers.multi_query import MultiQueryRetriever

multi_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm,
)

# ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
docs = multi_retriever.invoke("‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏µ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?")
```

### 2. Contextual Compression

‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏ï‡πà‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(),
)

docs = compression_retriever.invoke("‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ Q3 ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?")
# ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ Q3
```

### 3. Hybrid Search (‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô + Semantic)

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# Keyword search
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 4

# Semantic search
semantic_retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á 2 ‡πÅ‡∏ö‡∏ö
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, semantic_retriever],
    weights=[0.3, 0.7],  # ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å semantic ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
)
```

---

## ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF Q&A Bot

‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Python ‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ô‡∏µ‡πâ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def create_pdf_qa(pdf_path: str):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á Q&A bot ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF"""

    # 1. ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡πà‡∏á PDF
    docs = PyPDFLoader(pdf_path).load()
    chunks = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200
    ).split_documents(docs)

    print(f"üìÑ ‡πÇ‡∏´‡∏•‡∏î {len(docs)} ‡∏´‡∏ô‡πâ‡∏≤, ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô {len(chunks)} chunks")

    # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store
    vectorstore = Chroma.from_documents(
        chunks, OpenAIEmbeddings(model="text-embedding-3-small")
    )

    # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á RAG Chain
    prompt = ChatPromptTemplate.from_template("""
    ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

    ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {context}
    ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

    ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•):
    """)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    def format_docs(docs):
        return "\n\n---\n\n".join(
            f"[Source: {doc.metadata.get('source', 'unknown')}]\n{doc.page_content}"
            for doc in docs
        )

    chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | ChatOpenAI(model="gpt-4o-mini")
        | StrOutputParser()
    )

    return chain

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
qa_bot = create_pdf_qa("company_handbook.pdf")
answer = qa_bot.invoke("‡∏ß‡∏±‡∏ô‡∏•‡∏≤‡∏û‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏ô‡∏°‡∏µ‡∏Å‡∏µ‡πà‡∏ß‡∏±‡∏ô?")
print(answer)
```

---

:::note[‡∏Å‡πâ‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ ü§ñ]
‡πÑ‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **[‡∏ö‡∏ó‡∏ó‡∏µ‡πà 8: Agents ‡πÅ‡∏•‡∏∞ Tools](/03-advanced/02-agents-tools/)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á AI ‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏≠‡∏á!
:::
